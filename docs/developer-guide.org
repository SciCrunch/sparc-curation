#+TITLE: sparcur developer guide
#+AUTHOR: Tom Gillespie
#+OPTIONS: num:nil ^:nil h:7
#+LATEX_HEADER: \usepackage[margin=0.8in]{geometry}
#+STARTUP: showall

# [[file:developer-guide.pdf]]

* Demos
** Remote only connection
This is the simplest way to get a remote only connection to Blackfynn.
#+BEGIN_SRC python
from sparcur.paths import BlackfynnCache, Path
from sparcur.config import auth
from sparcur.backends import BlackfynnRemote
organization_id = auth.get('blackfynn-organization')
BlackfynnRemote = BlackfynnRemote._new(Path, BlackfynnCache)
BlackfynnRemote.init(organization_id)
root = BlackfynnRemote(BlackfynnRemote.root)
datasets = list(root.children)
return datasets
#+END_SRC
** Validate a dataset
You can run this example block and it will validate the DatasetTemplate. \\
To see the full results =from pprint import pprint=
and change the last line to =pprint(data)=.
#+BEGIN_SRC python :results output :exports both :cache yes :tangle ./broken.py
from sparcur import pipelines as pipes
from sparcur.paths import Path


def makeValidator(dataset_path):
    class context:
        path = dataset_path.resolve()
        id = path.id
        uri_api = path.as_uri()
        uri_human = path.as_uri()

    class lifters:
        id = context.id
        folder_name = context.path.name
        uri_api = context.uri_api
        uri_human = context.uri_human
        timestamp_export_start = None

    return pipes.SPARCBIDSPipeline(dataset_path, lifters, context)


path = Path('../resources/DatasetTemplate')
pipeline = makeValidator(path)
data = pipeline.data
print(sorted(data.keys()))
#+END_SRC

#+RESULTS[cc32c2f62cd7a758207c6368bc90a6742db681e3]:
: ['dirs', 'errors', 'files', 'id', 'inputs', 'meta', 'prov', 'size']

* Internal Structure
:PROPERTIES:
:header-args: :comments link :exports code
:END:
** Utility
#+begin_src python :tangle ../sparcur/simple/__init__.py :mkdirp yes :exports none
#+end_src

#+name: simple-utils
#+begin_src python :tangle ../sparcur/simple/utils.py
"""Common command line options for all sparcur.simple modules
Usage:
    sparcur-simple [options] [<path>...]

Options:
    -h --help                       show this

    --hypothesis-group-name=NAME    the hypotheis group name

    --project-id=ID                 the project id
    --dataset-id=ID                 one or more datset ids
    --project-id-auth-var=VAR       name of the auth variable holding the project-id

    --project-path=PATH             the project path
    --parent-path=PATH              the parent path of the
    --parent-parent-path=PATH       parent in which a random tempdir is generated
                                    to be the parent path, don't use this ...

    --jobs=N                        number joblib jobs [default: 12]
    --sparse-limit=N                package count that forces a sparse pull
    --symlink-objects=PATH          path to an existing objects directory
"""


from pyontutils import clifun as clif
from sparcur.paths import Path, BlackfynnCache
from sparcur.backends import BlackfynnRemote


def backend_blackfynn(Local=Path, Cache=BlackfynnCache):
    """ return a configured blackfynn backend
        calling this is sufficient to get everything set up correclty """

    RemotePath = BlackfynnRemote._new(Local, Cache)
    return RemotePath


class Options(clif.Options):

    @property
    def id(self):
        return self.project_id if self.project_id else self.dataset_id

    @property
    def jobs(self):
        return int(self._args['--jobs'])

    @property
    def paths(self):
        return [Path(p) for p in self._args['<path>']]

    @property
    def path(self):
        paths = self.paths
        if paths:
            return paths[0]


def pipe_main(main, after=None, argv=None):
    options, args, defaults = Options.setup(__doc__, argv=argv)
    out = main(**options.asKwargs())
    if after:
        after(out)

    return out
#+end_src
*** Test
#+begin_src python :tangle ../test/simple/test_utils.py :mkdirp yes
from sparcur.simple.utils import pipe_main

def test_pipe_main():
    def main(id, project_path, **kwargs):
        print(id, project_path, kwargs)

    pipe_main(main, argv=['sparcur-simple'])
#+end_src
** Pipelines
Easier to read, harder to debug. The python paradox.
*** Retrieve
**** Protocols
:PROPERTIES:
:header-args: :shebang "#!/usr/bin/env python3"
:END:
Cache annotations.
#+begin_src python :tangle ../sparcur/simple/fetch_annotations.py
from pathlib import Path
from hyputils import hypothesis as hyp
from sparcur.config import auth


def from_group_name_fetch_annotations(group_name):
    """ pull hypothesis annotations from remote to local """
    group_id = auth.user_config.secrets('hypothesis', 'group', group_name)
    cache_file = Path(hyp.group_to_memfile(group_id + 'sparcur'))
    get_annos = hyp.Memoizer(cache_file, group=group_id)
    get_annos.api_token = auth.get('hypothesis-api-key')  # FIXME ?
    annos = get_annos()
    return cache_file  # needed for next phase, annos are not


def main(hypothesis_group_name=None, **kwargs):
    if hypothesis_group_name is None:
        hypothesis_group_name = 'sparc-curation'

    from_group_name_fetch_annotations(hypothesis_group_name)


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main)
#+end_src
**** Datasets
:PROPERTIES:
:header-args: :shebang "#!/usr/bin/env python3"
:END:
***** Clone
This is an example of how to clone the top level of a project.
See ref:simple-utils for a good way to instantiate =RemotePath=.
#+name: simple-clone
#+begin_src python :tangle ../sparcur/simple/clone.py
from pathlib import Path


# clone top level
def from_path_id_and_backend_project_top_level(parent_path,
                                               project_id,
                                               RemotePath,
                                               symlink_objects_to=None):
    """ given the enclosing path to clone to, the project_id, and a fully
        configured (with Local and Cache) backend remote path, anchor the
        project pointed to by project_id along with the first level of children """

    RemotePath.init(project_id)  # calling init is required to bind RemotePath._api
    anchor = RemotePath.smartAnchor(parent_path)
    anchor.local_data_dir_init(symlink_objects_to=symlink_objects_to)
    list(anchor.children)
    project_path = anchor.local
    return project_path  # returned instead of anchor & children because it is needed by next phase


def main(parent_path=None,
         project_id=None,
         parent_parent_path=Path.cwd(),
         project_id_auth_var='blackfynn-organization',
         **kwargs):
    """ clone a project into a random subfolder of the current folder
        or specify the parent path to clone into """

    import tempfile
    from sparcur.config import auth
    from sparcur.simple.utils import backend_blackfynn

    if parent_path is None:
        parent_path = Path(tempfile.mkdtemp(dir=parent_parent_path))

    project_id = auth.get(project_id_auth_var)
    RemotePath = backend_blackfynn()
    symlink_objects_to = None  # TODO
    project_path = from_path_id_and_backend_project_top_level(
        parent_path,
        project_id,
        RemotePath,
        symlink_objects_to,)

    return project_path


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main, after=print)
#+end_src
***** Remote metadata
Remote metadata must be retrieved prior to the first pull
in order to ensure that large datasets can be marked as
sparse datasets before they are pulled.
****** From id
Remote metadata can be retrieved using only a project_id. However,
for all retrieval after the first pull it is usually more effective
to retrieve it at the same time as fetching metadata files since it
runs in parallel per dataset.
#+begin_src python :tangle ../sparcur/simple/fetch_remote_metadata_all.py
from joblib import Parallel, delayed
from sparcur.backends import BlackfynnDatasetData
from sparcur.simple.utils import backend_blackfynn


def from_id_fetch_remote_metadata(id, n_jobs=12):
    """ given an dataset id fetch its associated dataset metadata """
    if id.startswith('N:organization'):
        RemotePath = backend_blackfynn()
        project = RemotePath(id)
        prepared = [BlackfynnDatasetData(r) for r in project.children]
        if n_jobs <= 1:
            [p() for p in prepared]
        else:
            Parallel(n_jobs=12)(delayed(p)() for p in prepared)
    elif id.startswith('N:dataset'):
        bdd = BlackfynnDatasetData(id)
        bdd()
    else:
        raise NotImplementedError(id)


def main(id=None, n_jobs=12, **kwargs):
    if id is None:
        from sparcur.config import auth
        id = auth.get('blackfynn-organization')

    from_id_fetch_remote_metadata(id, n_jobs=n_jobs)


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main)
#+end_src
****** From path
:PROPERTIES:
:CUSTOM_ID: fetch-remote-metadata
:END:
The implementation of =sparcur.backends.BlackfynnDatasetData= supports the ability
to retrieve metadata directly from the remote without the need for an intervening
local path. However this functionality is obscured here because we want to derive
a consistent view of the data from the file system snapshot.
#+begin_src python :tangle ../sparcur/simple/fetch_remote_metadata.py
from joblib import Parallel, delayed
from sparcur.paths import Path
from sparcur.backends import BlackfynnDatasetData


def _from_project_path_fetch_remote_metadata(project_path, n_jobs=12, cached_ok=False):
    if n_jobs <= 1:
        prepared = [BlackfynnDatasetData(dataset_path.cache)
                    for dataset_path in project_path.children]
        [bdd() for bdd in prepared if not (cached_ok and bdd.cache_path.exists())]
    else:
        fetch = lambda bdd: bdd() if not (cached_ok and bdd.cache_path.exists()) else None
        fetch_path = (lambda path: fetch(BlackfynnDatasetData(path.cache)))
        Parallel(n_jobs=n_jobs)(delayed(fetch_path)(dataset_path)
                 for dataset_path in project_path.children)


# fetch remote metadata
def from_path_fetch_remote_metadata(path, n_jobs=12, cached_ok=False):
    """ Given a path fetch remote metadata associated with that path. """

    cache = path.cache
    if cache.is_organization():
        _from_project_path_fetch_remote_metadata(path, n_jobs=n_jobs, cached_ok=cached_ok)
    else:  # dataset_path
        # TODO more granular rather than roll up to dataset if inside?
        bdd = BlackfynnDatasetData(cache)
        if not (cached_ok and bdd.cache_path.exists()):
            bdd()


def main(path=Path.cwd(), n_jobs=12, rmeta_cached_ok=False, **kwargs):
    if path is None or path.find_cache_root() not in (path, *path.parents):
        from sparcur.simple.clone import main as clone
        path = clone(**kwargs)

    from_path_fetch_remote_metadata(path, n_jobs=n_jobs, cached_ok=rmeta_cached_ok)
    return path


if __name__ == '__main__':
    path = main()
    print(path)
#+end_src
***** Pull
Pull a single dataset or pull all datasets or clone and pull all datasets.
#+begin_src python :tangle ../sparcur/simple/pull.py
from joblib import Parallel, delayed
from sparcur.paths import Path
from sparcur.utils import GetTimeNow


# pull dataset
def from_path_dataset_file_structure(path, time_now=None):
    """ pull the file structure and file system metadata for a single dataset
        right now only works from a dataset path """

    if time_now is None:
        time_now = GetTimeNow()

    path._pull_dataset(time_now)


# pull all in parallel
def from_path_dataset_file_structure_all(project_path, time_now=None):
    """ pull all of the file structure and file system metadata for a project """
    if time_now is None:
        time_now = GetTimeNow()

    project_path.pull(
        time_now=None,  # TODO
        debug=False,  # TODO
        n_jobs=12,
        log_level='DEBUG' if False else 'INFO',  # TODO
        Parallel=Parallel,
        delayed=delayed,)


# mark datasets as sparse 
def sparse_materialize(path, sparse_limit=None):
    """ given a path mark it as sparse if it is a dataset and
        beyond the sparse limit """

    cache = path.cache
    if cache.is_organization():
        # don't iterate over cache children because that pulls remote data
        for child in path.children:
            sparse_materialize(child, sparse_limit=sparse_limit)
    else:
        cache._sparse_materialize(sparse_limit=sparse_limit)


def main(path=Path.cwd(), time_now=None, sparse_limit=None, **kwargs):
    project_path = None
    if path is None or path.find_cache_root() not in (path, *path.parents):
        from sparcur.simple.fetch_remote_metadata import main as remote_metadata
        project_path = remote_metadata(**kwargs)
    else:
        project_path = path.find_cache_root()
        if path != project_path:
            # dataset_path case
            sparse_materialize(path, sparse_limit=sparse_limit)
            from_path_dataset_file_structure(path)
            print('NOTE: you probably need to run `pushd ~/ && popd` '
                'to get a sane view of the filesystem if you ran this'
                'from within a dataset folder')
            return path

    sparse_materialize(project_path, sparse_limit=sparse_limit)
    from_path_dataset_file_structure_all(project_path)
    return project_path


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main, after=print)
#+end_src
***** Fetch
#+begin_src python :tangle ../sparcur/simple/fetch.py
from sparcur.simple.fetch_metadata_files import main as files
from sparcur.simple.fetch_remote_metadata import main as rmeta


def main(path=Path.cwd(), **kwargs):
    if path is None or not path.find_cache_root() in (path, *path.parents):
        from sparcur.simple.pull_all import main as pull
        path = pull(**kwargs)

    # FIXME these can be run in parallel
    # python is not its own best glue code ...
    rmeta(path=path)
    files(path=path)
    return path


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main, after=print)
#+end_src
****** Metadata files
# ugh I gave myself the name in a loop variable colliding with
# name at higher level of indentation still in a loop bug, so
# totally will overwrite the name and cause madness to ensue
#+begin_src python :tangle ../sparcur/simple/fetch_metadata_files.py
from joblib import Parallel, delayed
from sparcur import exceptions as exc
from sparcur.utils import log, logd
from sparcur.paths import Path
from sparcur.datasets import DatasetStructure

# fetch metadata files
fetch_prefixes = (
    'dataset_description',
    'subjects',
    'samples',
    'submission',
    'manifest',
)


def _from_path_fetch_metadata_files_simple(path, fetch=True):
    """ transitive yield paths to all metadata files, fetch them from
        the remote if fetch == True """
    for glob_prefix in fetch_prefixes:
        ds = DatasetStructure(path)
        for path_to_metadata in ds._abstracted_paths(glob_prefix, fetch=fetch):
            yield path_to_metadata


def _from_path_fetch_metadata_files_parallel(path, n_jobs=12):
    """ Fetch all metadata files within the current path in parallel. """
    paths_to_fetch = list(_from_path_fetch_metadata_files_simple(path, fetch=False))
    if not len(paths_to_fetch):
        log.warning('No paths to fetch, did you pull the file system metadata?')

    def fetch(cache):
        # lambda functions are great right up until you have to handle an
        # error function inside of them ... thanks python for yet another
        # failure to be homogenous >_<
        meta = cache.meta
        try:
            size_mb = meta.size.mb
        except AttributeError as e:
            if meta.errors:
                logd.debug(f'remote errors {meta.errors} for {cache!r}')
                return
            else:
                raise e

        return cache.fetch(size_limit_mb=size_mb + 1)

    fetch_path = lambda path: fetch(path.cache)
    Parallel(n_jobs=n_jobs)(delayed(fetch_path)(path) for path in paths_to_fetch)


def from_path_fetch_metadata_files(path, n_jobs=12):
    """ fetch metadata files located within a path """
    if n_jobs <= 1:
        _from_path_fetch_metadata_files_simple(path)
    else:
        _from_path_fetch_metadata_files_parallel(path, n_jobs=n_jobs)


def main(path=Path.cwd(), n_jobs=12, **kwargs):
    if path is None or path.find_cache_root() not in (path, *path.parents):
        from sparcur.simple.pull_all import main as pull
        path = pull(**kwargs)

    from_path_fetch_metadata_files(path, n_jobs=n_jobs)
    return path


if __name__ == '__main__':
    from sparcur.simple.utils import pipe_main
    pipe_main(main)
#+end_src
****** unused :noexport:
#+begin_src python
from_id_remote_metadata = lambda id: ds.BlackfynnDatasetData(id)()
compose = lambda f, g: (lambda *x: f(g(*x)))
#from_path_remote_metadata = compose(lambda id: from_id_remote_metadata(id),
                                    #lambda path: path.cache.id)
#+end_src
*** Validate
**** Protocols
**** Datasets
#+begin_src python
from sparcur.paths import Path
from sparcur.datasets import DatasetStructure


def from_path_summary(project_path):
    dataset_path_structure
    summary((
        dataset(
            dataset_path_structure
            dataset_description
            subjects
            samples
            submission
            manifests
            *rest
)))


#def dataset(path_structure, description, subjects, samples, submission, manifests, *rest):
def dataset(*objects):
    data = {}
    #path_structure, description, subjects, samples, submission, manifests, *rest = objects
    for obj in objects:
        data.update(obj.data)  # FIXME this seems too naievely simple

    return data


def from_path_dataset(dataset_path):
    return dataset(*comb_dataset(dataset_path))


def object_from_find_path(glob_prefix, object_from_path_function, glob_type='glob'):
    if glob_prefix not in fetch_prefixes:
        raise ValueError('glob_prefix not in fetch_prefixes! '
                         f'{glob_prefix!r} not in {fetch_prefixes}')
    def func(path, *args, **kwargs):
        ds = DatasetStructure(path)
        for path in ds._abstracted_paths(glob_prefix, sandbox=True):
            yield object_from_path_function(path, *args, **kwargs)

    return func


# TODO how to attach and validate schemas orthogonally in this setting?
# e.g. so that we can write dataset_1_0_0 dataset_1_2_3 etc.
# FIXME it is never this simple :/ have to dispatch on template version
# which we can only know at runtime
def description(path): return dat.DatasetDescriptionFilePath(path).object

def submission(path):  return dat.SubmissionFilePath(path).object
def subjects(path):    return dat.SubjectsFilePath(path).object
def samples(path):     return dat.SamplesFilePath(path).object
def manifest(path):    return dat.ManifestFilePath(path).object


def from_path_dataset_path_structure(path):
    return


from_path_dataset_description = object_from_find_path('dataset_description', description)
from_path_subjects            = object_from_find_path('subjects',            subjects)
from_path_samples             = object_from_find_path('samples',             samples)
from_path_submission          = object_from_find_path('submission',          submission)
from_path_manifests           = object_from_find_path('manifest',            manifest, 'rglob')
from_path_remote_metadata     = lambda path: ds.BlackfynnDatasetData(path.cache).fromCache()


def combinate(*functions):
    def combinator(*args, **kwargs):
        for f in functions:
            yield f(*args, **kwargs)

    return combinator


# this is all well and good right up until the moment that
# the rest of these depend on one of the others
comb_dataset = combinate(
    from_path_dataset_path_structure,
    #from_path_dataset_description,  # must come first
    from_path_subjects,
    from_path_samples,
    from_path_submission,
    from_path_manifests,
    from_path_remote_metadata,)


def from_export_path_protocols_io_data(curation_export_json_path): pass
def protocols_io_ids(datasets): pass
def protocols_io_data(protocols_io_ids): pass

def from_group_name_protcur(group_name): pass
def protcur_output(): pass

def summary(datasets, protocols_io_data, protcur_output): pass


def main(path=Path.cwd()):
    dataset = from_path_dataset(path)
    breakpoint()


if __name__ == '__main__':
    main()
#+end_src
**** Network resources
*** Export
* Workflow
** All datasets
*** Retrieve                                                         :ignore:
**** Overview                                                        :ignore:
The dependency DAG is as follows.
# NOTE the workflow for generating these diagrams takes multiple steps
# first write the graph in racket, where we can use dashes in names
# conver to dot and add clusters as needed
#+name: graph-retrieve-all
#+header: :wrap "src dot :file ./images/graph-retrieve-all.png :cmdline -Kdot -Tpng :exports results :cache yes"
#+begin_src racket :lang racket/base :exports none :noweb no-export :cache yes
<<racket-graph-helper>>
(define g (dag-notation
           fetch-all -> fetch-annotations
           fetch-all -> fetch-metadata-files
           fetch-all -> fetch-remote-metadata
           fetch-metadata-files -> pull
           pull -> sparse-materialize
           sparse-materialize -> fetch-remote-metadata
           pull -> clone))

(graphviz g)
#+end_src

#+RESULTS[e29666f9fbc8046ebab291bb30916242ab20cfeb]: graph-retrieve-all
#+begin_src dot :file ./images/graph-retrieve-all.png :cmdline -Kdot -Tpng :exports results :cache yes
digraph G {
	node0 [label="fetch-metadata-files"];
	node1 [label="clone"];
	node2 [label="fetch-all"];
	node3 [label="fetch-remote-metadata"];
	node4 [label="pull"];
	node5 [label="fetch-annotations"];
	node6 [label="sparse-materialize"];
	subgraph cluster_F {
        color=none;
        node2;
	}
	subgraph cluster_D {
        label="Dataset";
        color=green;
		node0 -> node4;
		node2 -> node0;
		node2 -> node3;
		node4 -> node1;
		node4 -> node6;
		node6 -> node3;
	}
	subgraph cluster_P {
        label="Protcur";
        color=purple;
		node2 -> node5;
    }
}
#+end_src

#+RESULTS[041cb032103aa97b8bf26a7397abce2271f685b5]:
[[file:./images/graph-retrieve-all.png]]
**** Bash implementation                                             :ignore:
# FIXME this is really an env file not a bin file ...
#+begin_src bash :tangle ../bin/pipeline-functions.sh :mkdirp yes
function sparcur-get-all-remote-data () {
    # NOTE not quite all the remote data, the google sheets
    # don't have caching functionality yet

    # TODO parse args
    local TIME_START=$(date -In)
    mv "$(mktemp --directory sparcur-all-XXXXXX)" "${TIME_START}" || \
        { CODE=$?; echo 'mv failed'; return $CODE; }
    local PARENT_PATH=${TIME_START}
    local LOG_PATH="${PARENT_PATH}/logs"
    #local LOG_PATH=$(python -c "from sparcur.config import auth; print(auth.get_path('log-path'))")
    local PROJECT_ID=$(python -c "from sparcur.config import auth; print(auth.get('blackfynn-organization'))")
    local SYMLINK_OBJECTS_TO=$()

    echo ${PARENT_PATH}  # needed to be able to follow logs

    if [ ! -d "${LOG_PATH}" ]; then
        mkdir "${LOG_PATH}"
    fi

    # fetch annotations
    python -m sparcur.simple.fetch_annotations > "${LOG_PATH}/fetch-annotations.log" 2>&1 &
    local pids_final[0]=$!

    # fetch remote metadata
    python -m sparcur.simple.fetch_remote_metadata_all \
           --project-id ${PROJECT_ID} \
           > "${LOG_PATH}/fetch-remote-metadata.log" 2>&1 &
    local pids[0]=$!

    # clone aka fetch top level
    local FAIL=0
    local PROJECT_PATH=$(python -m sparcur.simple.clone \
                                --project-id ${PROJECT_ID} \
                                --parent-path "${PARENT_PATH}" \
                                > "${LOG_PATH}/clone.log" 2>&1) || FAIL=$((FAIL+1))

    for pid in ${pids[*]}; do
        wait $pid || FAIL=$((FAIL+1))
    done
    if [[ $FAIL -ne 0 ]]; then
        echo "${FAIL} commands failed. Cannot continue."
        echo "${PROJECT_PATH}"
        return 1
    fi

    # pull aka fetch file system metadata
    python -m sparcur.simple.pull \
           --project-path "${PROJECT_PATH}" \
           > "${LOG_PATH}/pull.log" 2>&1 || { CODE=$?;
                                              echo 'pull failed!';
                                              echo "${PROJECT_PATH}";
                                              return $CODE; }

    # fetch metadata files
    python -m sparcur.simple.fetch_metadata_files \
           --project-path "${PROJECT_PATH}" \
           > "${LOG_PATH}/fetch-metadata-files.log" 2>&1 &

    pids_final[1]=$!
    local FAIL=0
    for pid in ${pids_final[*]}; do
        wait $pid || FAIL=$((FAIL+1))
    done
    if [[ $FAIL -ne 0 ]]; then
        echo "${FAIL} commands failed. Cannot continue."
        echo "${PROJECT_PATH}"
        return 1
    fi
}
#+end_src
*** Validate                                                         :ignore:
This is the graph of the existing approach. A slightly more sane version
is implemented above and tangled as =sparcur.simple=.

# runs both but I'm fairly cerain that it fails to update the second code block
# #+name: graph-validate-run-both
# #+begin_src elisp :var one=graph-validate-all() two=graph-validate-all-dot() :results none
# #+end_src

#+name: graph-validate-all
#+header: :wrap "src dot :file ./images/graph-validate-all.png :cmdline -Kdot -Tpng :exports results :cache yes"
#+begin_src racket :lang racket/base :exports none :noweb no-export :cache yes
<<racket-graph-helper>>
(define g (dag-notation
           ; I had description listed depending on dataset-structure
           ; but that is really an implementation detail

           pipeline-end -> pipeline-extras -> sparc-ds -> pipeline-start -> description -> fetch-all
                                                          pipeline-start -> dataset-structure -> fetch-all
                                                          pipeline-start -> dataset-metadata -> fetch-all

                                              ; note that this is the idealized flow
                                              ; the actual flow is through pipeline-start
                                              sparc-ds -> submission -> description
                                                          submission -> fetch-all
                                              sparc-ds -> subjects -> description
                                                          subjects -> fetch-all
                                              sparc-ds -> samples -> description
                                                          samples -> fetch-all
                                              sparc-ds -> manifest -> description
                                                          manifest -> fetch-all

                           pipeline-extras -> submission-normalized -> submission
                           pipeline-extras -> contributors -> affiliations #;lifters -> affiliations-sheet -> sheets -> network
                                              contributors -> member #;state -> blackfynn-api -> network
                                              contributors -> description
                           pipeline-extras -> meta-extra -> organ-term #;lifters -> organs-sheet -> sheets
                                              meta-extra -> modality #;lifters -> organs-sheet
                                              meta-extra -> techniques #;lifters -> organs-sheet
                                              meta-extra -> protocol-uris #;lifters -> organs-sheet
                                              meta-extra -> award-manual #;lifters -> organs-sheet
                                              meta-extra -> dataset-doi -> blackfynn-api ; path.cache.remote.bfobject.doi ; what a hack :x
                                              ; FIXME TODO this is how dataset-doi _should_ work
                                              ; uncomment when it _actually_ works like this
                                              ; meta-extra -> dataset-doi -> cache-remote-metadata -> fetch-all
                                              meta-extra -> award-organ #; lifters -> submission-normalized
                                                            award-organ -> scraped-award-organ
           ??? -> overview-sheet -> sheets))

;; subgraphs
(define lifters '(affiliations organ-term modality techniques protocol-uris award-manual award-organ))
(define state '(member))
(define network '(network blackfynn-api sheets affiliation-sheet organs-sheet overview-sheet))

(define-vertex-property g vertex-id #:init $id)  ; doesn't work to get the graphviz node numbering

(define-vertex-property g in-lifters?)
(for-each (位 (v) (in-lifters?-set! v #t)) lifters)

(define-vertex-property g in-state?)
(for-each (位 (v) (in-state?-set! v #t)) state)

(define-vertex-property g in-network?)
(for-each (位 (v) (in-network?-set! v #t)) network)

(graphviz g)
#+end_src

#+name: graph-validate-all-dot
#+RESULTS[16bcd2566c9bc6aca9c4c547144fe50c5a542558]: graph-validate-all
#+begin_src dot :file ./images/graph-validate-all.png :cmdline -Kdot -Tpng :exports results :cache yes
digraph G {
	node0 [label="description"];
	node1 [label="modality"];
	node2 [label="dataset-doi"];
	node3 [label="blackfynn-api"];
	node4 [label="dataset-metadata"];
	node5 [label="samples"];
	node6 [label="subjects"];
	node7 [label="award-manual"];
	node8 [label="submission-normalized"];
	node9 [label="organs-sheet"];
	node10 [label="scraped-award-organ"];
	node11 [label="member"];
	node12 [label="sheets"];
	node13 [label="award-organ"];
	node14 [label="network"];
	node15 [label="submission"];
	node16 [label="fetch-all"];
	node17 [label="manifest"];
	node18 [label="techniques"];
	node19 [label="overview-sheet"];
	node20 [label="pipeline-extras"];
	node21 [label="pipeline-end"];
	node22 [label="pipeline-start"];
	node23 [label="protocol-uris"];
	node24 [label="affiliations"];
	node25 [label="affiliations-sheet"];
	node26 [label="contributors"];
	node27 [label="organ-term"];
	node28 [label="meta-extra"];
	node29 [label="dataset-structure"];
	node30 [label="sparc-ds"];
	node31 [label="???"];
	subgraph U {
		edge [dir=none];
	}
	subgraph cluster_M {
		label="Metadata Files";
		color=green;
        node0;
        node5;
        node6;
        node15;
        node17;
	}
	subgraph cluster_L {
		label="Lifters (bad design)";
		color=red;
        node1;
        node7;
        node13;
        node18;
        node24;
        node23;
        node27;
	}
	subgraph D {
		node0 -> node16;
		node1 -> node9;
		node2 -> node3;
		node3 -> node14;
		node4 -> node16;
		node5 -> node0;
		node5 -> node16;
		node6 -> node0;
		node6 -> node16;
		node7 -> node9;
		node8 -> node15;
		node9 -> node12;
		node11 -> node3;
		node12 -> node14;
		node13 -> node10;
		node13 -> node8;
		node15 -> node0;
		node15 -> node16;
		node17 -> node0;
		node17 -> node16;
		node18 -> node9;
		node19 -> node12;
		node20 -> node30;
		node20 -> node28;
		node20 -> node26;
		node20 -> node8;
		node21 -> node20;
		node22 -> node0;
		node22 -> node29;
		node22 -> node4;
		node23 -> node9;
		node24 -> node25;
		node25 -> node12;
		node26 -> node0;
		node26 -> node11;
		node26 -> node24;
		node27 -> node9;
		node28 -> node1;
		node28 -> node2;
		node28 -> node18;
		node28 -> node13;
		node28 -> node23;
		node28 -> node7;
		node28 -> node27;
		node29 -> node16;
		node30 -> node17;
		node30 -> node5;
		node30 -> node15;
		node30 -> node22;
		node30 -> node6;
		node31 -> node19;
	}
}
#+end_src

#+RESULTS[20008f92af2cbbe5a5aa89221885829ea3bd0f11]: graph-validate-all-dot
[[file:./images/graph-validate-all.png]]

*** Export                                                           :ignore:
** Single dataset
*** Retrieve                                                         :ignore:
**** Overview                                                        :ignore:
**** Bash implementation                                             :ignore:
#+begin_src bash
DATASET_PATH=$(python -m sparcur.simple.clone --dataset-id ${DATASET_ID})
python -m sparcur.simple.fetch_remote_metadata ${DATASET_PATH}
python -m sparcur.simple.pull ${DATASET_PATH}
python -m sparcur.simple.fetch_metadata_files ${DATASET_PATH}
#+end_src
*** Validate                                                         :ignore:
*** Export                                                           :ignore:
* Code :noexport:
See also https://docs.racket-lang.org/graphviz/index.html =raco pkg install racket-graphviz=
for more direct mapping of graphviz functionality but one that is also way more verbose.
#+name: racket-graph-helper
#+header: :prologue "#lang racket/base"
#+begin_src racket :lang racket/base :exports none :tangle ./y-u-no-compile-from-buffer.rkt :tangle no
(require graph ; rack pkg install graph
         (only-in racket/string
                  string-trim
                  string-replace)
         (for-syntax racket/base
                     syntax/parse))

(define-for-syntax (list-to-pairs l)
  (for/list ([a l] [b (cdr l)]) (list a b)))

(define-syntax (dag-notation stx)
  (syntax-parse stx
    #:datum-literals (->)
    [(_ (~seq left (~seq -> right) ...) ...)
     #:with (pairs ...) (datum->syntax this-syntax (apply append (map list-to-pairs (syntax->datum #'((left right ...) ...)))))
     #'(unweighted-graph/directed (quote (pairs ...)))]))

(define (subgraph->graphviz subgraph->hash)
  (let ([members (for/list ([(k v) (in-hash (subgraph->hash))] #:when v) k)]
        [label (string-replace (string-trim (symbol->string (object-name subgraph->hash)) "->hash")
                               #rx"[-_?>]"
                               "_")])
    (string-append (format "subgraph cluster_~a" label)
                   ; FIXME this won't quite work because we need to know
                   ; the ids to which the nodes were assigned :/
                   )
    ))

(define (graphviz-subgraphs graph #:subgraphs [subgraph->hash-functions '()])
  "wrap graphviz since it is too simple for our needs at the moment
subgraphs should be specified using vertext properties or edge properties"
  ;; XXX really more clusters
  (define s (graphviz graph))
  (let* ([sl (string-length s)]
         [split-at (- sl 2)]
         [start (substring s 0 split-at)]
         [end (substring s split-at sl)]
         [extra (map subgraph->graphviz subgraph->hash-functions)])
         (apply string-append `(,start ,@extra ,end))))

(module+ test
  (require racket/pretty)
  (define g (dag-notation a -> b -> c
                          b -> d -> e -> f))
  (pretty-print g)
  (graphviz g)

  (define-vertex-property g my-subgraph)
  (for-each (位 (v) (my-subgraph-set! v #t)) '(b c d))

  (define sgh (list my-subgraph->hash))
  (graphviz-subgraphs g #:subgraphs sgh)
)

(module+ test
  ; TODO explor possibility of using -^ or -> ^ or | syntax
  ; to point up to the most recent definition chain containing
  ; the start of the chain in question and having one more
  ; elment than the current chain

  #;
  (dag-notation
   ; for example
   a -> b -> c -> d
        b -> q |
        b -> p |

   ; expands to
   a -> b -> c -> d
        b -> q -> d
        b -> p -> d)

  ; in theory this notation could also be used in reverse, but I'm worried about
  ; accidental hard to debug errors if a line accidentally ends with an arrow

  #;
  (dag-notation
   ; clearly confusing
   a -> b -> c -> d
   | -> d
        | -> e
   ; this would probably read as
   a -> b -> c -> d
   a -> d
   a -> e
   ; hrm
   a -> b -> c -> d
             a |
   | e ; not sure if I like this pretty sure I dont ...
   )
  )
#+end_src
