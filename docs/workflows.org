#+TITLE: SPARC workflows
#+AUTHOR: Tom Gillespie
#+property: header-args :eval no-export

* SPARC
** WARNINGS
1. *DO NOT USE* =cp -a= copy files with xattrs! \\
   *INSTEAD* use =rsync -X -u -v=. \\
   =cp= does not remove absent fields from xattrs of the file previously
   occupying that name! OH NO (is this a =cp= bug!?)
** Export v5
:PROPERTIES:
:CUSTOM_ID: export-v5
:END:
#+begin_src bash
source ~/files/venvs/sparcur-dev/bin/activate
python -m sparcur.simple.combine
echo Export complete. Check results at:
echo https://cassava.ucsd.edu/sparc/preview/archive/summary/$(readlink ~/.local/share/sparcur/export/summary/618*/LATEST)
#+end_src
** Rerun single datasets on production
One or more values for =dataset_id= can be provided as =uuid=, =N:dataset:uuid=, or =dataset:uuid=.
#+begin_src bash
docker exec --user 836 -it $(docker ps -lqf ancestor=tgbugs/musl:sparcron-user) pypy3 -m sparcur.sparcron.rerun ${dataset_id}
#+end_src
See example in [[https://github.com/tgbugs/dockerfiles/blob/master/source.org#an-example-of-how-to-rerun-one-or-more-datasets][dockerfiles source]] for more.
** Report
#+begin_src bash :eval never
function fetch-and-run-reports () {
    local FN="/tmp/curation-export-$(date -Is).json"
    curl https://cassava.ucsd.edu/sparc/preview/exports/curation-export.json -o "${FN}"
    spc sheets update Organs --export-file "${FN}"
    spc report all --sort-count-desc --to-sheets --export-file "${FN}"
}
fetch-and-run-reports
#+end_src
*** COMMENT deprecated
You can't run this directly because the venvs create their own subshell.
#+begin_src bash :dir "/ssh:cassava-sparc:~/files/test2/SPARC Curation" :eval never
# git repos are in ~/files/venvs/sparcur-dev/git
# use the development pull code
source ~/files/venvs/sparcur-dev/bin/activate
spc pull
# switch to the production export pipeline
source ~/files/venvs/sparcur-1/bin/activate
spc export
#+end_src

#+begin_src bash :dir /ssh:cassava|sudo:cassava
<<&sparc-export-to-server-function>>
sparc-export-to-server
#+end_src
** Reporting
:PROPERTIES:
:VISIBILITY: folded
:END:
turtle diff
#+begin_src bash
spc report changes \
--ttl-file https://cassava.ucsd.edu/sparc/preview/archive/exports/2021-05-25T125039,817048-0700/curation-export.ttl \
--ttl-compare https://cassava.ucsd.edu/sparc/preview/archive/exports/2021-05-24T141309,920776-0700/curation-export.ttl
#+end_src
#+CAPTION: reports
#+BEGIN_SRC bash
spc report completeness
#+END_SRC

#+CAPTION: reporting dashboard
#+BEGIN_SRC bash
spc server --latest --count
#+END_SRC

#+begin_src python
keywords = sorted(set([k for d in asdf['datasets'] if 'meta' in d and 'keywords' in d['meta']
                       for k in d['meta']['keywords']]))
#+end_src
** Compacting dataset exports
The proper way to do this is to compact _all_ exports as soon as they
are created (though in a separate process). Then run compact to remove
folders that are no longer needed since everything will be xzed from
the start.

To xz folders that have not yet been compressed (e.g. to backfill in
cases where only a subset were compacted prior to updating to the new
process) run the following.

#+begin_src bash :eval never
find /var/lib/sparc/.local/share/sparcur/export/datasets/ \
-maxdepth 2 -mindepth 2 -type d -not -exec test -f '{}.tar.xz' \; -print | \
xargs -P4 -r -I{} sh -c 'XZ_OPT=-e9 tar -C "{}" -cvJf "{}.tar.xz" .'
#+end_src

Once all dataset exports have been archived it is safe to run the following.
#+begin_src bash :eval never
python -m sparcur.simple.compact | xargs -P4 -r -I{} sh -c 'rm -r "{}"'
#+end_src

See [[./developer-guide.org::#compact]] for the old workflow.

TODO Implement =sparcur.simple.compact_snapshot= to elink older
snapshots to the archives so the folders can be removed. Need to
figure out whether we need a different naming convention or a way
to indicate that a snapshot points to archived data ... possibly
by adding an extension to the uuid, or maybe the content type will
be sufficient?

** Archive pull index
#+begin_src bash
#set -e -o pipefail

function archive-pull-index () {
local d dp toarch archname torm
pushd ~/files/sparc-datasets-test
for d in $(ls); do
    dp=${d}/SPARC/.operations/index/pull/${d}
    if [ -d ${dp} ]; then
        pushd ${dp}
        toarch=$(ls *Z)
        archname=archive-through-$(echo ${toarch} | awk '{ print $NF }').tar.xz
        torm=$(echo ${toarch} | awk 'BEGIN{FS=OFS=" "}{NF--; print}')
        tar c ${toarch} | \
        xz -9e > "${archname}" && \
        [ "${torm}" ] && rm ${torm}
        popd
    fi
done
popd
}
#+end_src

** Dataset modification statistics
I'm sure there is another way to get this data from pennsieve
that would make more sense and that would make it possible to
distinguish curation changes vs user changes, but this was the
quick and dirty.

#+name: el-def-helper-funs
#+begin_src elisp :results none
(defun --fn-sparcur-prod-dataset-mod-dates ()
  ;; (fmakunbound '--fn-sparcur-prod-dataset-mod-dates)
  (concat "sparcur-prod-dataset-mod-dates-"
          (format-time-string "%Y-%m-%d" (current-time))
          ".tsv"))
(defun --fn-sparcur-prod-dataset-mod-dates-types ()
  ;; (fmakunbound '--fn-sparcur-prod-dataset-mod-dates-types)
  (concat "sparcur-prod-dataset-mod-dates-types-"
          (format-time-string "%Y-%m-%d" (current-time))
          ".tsv"))
#+end_src

# uhhhhh ... oh right, we haven't actually fixed org babel header elisp security yet
#+header: :file (let (org-confirm-babel-evaluate) (org-sbe "el-def-helper-funs") (expand-file-name (--fn-sparcur-prod-dataset-mod-dates)))
#+name: remote-result
#+begin_src bash :dir /ssh:cassava:/var/lib/sparc/.local/share/sparcur/export/datasets/ :results file
find -mindepth 2 -maxdepth 2 | awk -F'/' '{ print $2 "\t" $3 }' | sed 's/\.tar\.xz$//' | grep -v LATEST | sort
#+end_src

This command takes a long time run and therefore should be run separately copied back.
Before running this all folders, even those in snapshots, should be tarred to simplify processing.
#+begin_src bash :eval never :dir /ssh:cassava:/var/lib/sparc/.local/share/sparcur/export/datasets/
find -name '*.tar.xz' -exec sh -c 'echo $(echo "${1}" | awk -F/ '"'"'{ print $2 " " $3 }'"'"' | sed '"'"s/\.tar\.xz-new$//"'"') $(tar -xvJOf "${1}" ./path-metadata.json 2>/dev/null | jq -r ".data | sort_by(.timestamp_updated) | reverse[0] | if .dataset_relative_path == \"\" then (.mimetype + \" \" + \"dataset\") else ((if .mimetype == null then \"?\" else .mimetype end) + \" \" + (if .basename == null then \"?\" else .basename end)) end")' _ {} \; | sed 's/ /\t/g' > /tmp/prod-object-events-$(date -I).tsv
#+end_src

#+header: :file (let (org-confirm-babel-evaluate) (org-sbe "el-def-helper-funs") (expand-file-name (--fn-sparcur-prod-dataset-mod-dates-types)))
#+begin_src bash :dir /ssh:cassava:/var/lib/sparc/.local/share/sparcur/export/datasets/ :results file
cat /tmp/prod-object-events*.tsv | sort -u
#+end_src

** Archiving files with xattrs
:PROPERTIES:
:VISIBILITY: folded
:END:
=tar= is the only one of the 'usual' suspects for file archiving that
supports xattrs, =zip= cannot.

#+CAPTION: archive
#+begin_src bash
tar --force-local --xattrs -cvzf 2019-07-17T10\:44\:16\,457344.tar.gz '2019-07-17T10:44:16,457344/'
#+end_src

#+CAPTION: extract
#+begin_src bash
tar --force-local --xattrs -xvzf 2019-07-17T10\:44\:16\,457344.tar.gz
#+end_src

#+CAPTION: test
#+begin_src bash
find 2019-07-17T10\:44\:16\,457344 -exec getfattr -d {} \;
#+end_src
** Archiving releases
*** TODO consider zstd vs xz?
:PROPERTIES:
:CREATED:  [2023-01-18 Wed 16:01]
:END:
#+begin_src bash
tar -cvf --zstd asdf.zst asdf
#+end_src
#+begin_example
-rw-r--r--  1 sparc sparc  95M Jan 21 19:50 2023-01-20T123933,576391-0800.tar.gz
-rw-r--r--  1 sparc sparc  60M Jan 21 19:43 2023-01-20T123933,576391-0800.tar.xz
-rw-r--r--  1 sparc sparc  82M Jan 21 19:49 2023-01-20T123933,576391-0800.tar.zst
#+end_example
so ... zstd is faster than gzip by a wide margin ... like ... way faster
xz still wins on the ratio though, for background stuff that isn't time
sensitive, better to go higher ratio
*** in place
Manually remove the echo after checking that you are removing what you expect.
#+begin_src bash
pushd /var/www/sparc/sparc/
    pushd archive/exports
        find -maxdepth 1 -not -path '.' -type d -exec tar -cvJf '{}.tar.xz' '{}' \;
        chown nginx:nginx *.tar.xz
        # remove all but the one currently symlinked to exports
        find -maxdepth 1 -not -path '.' -not -path "*$(basename $(readlink ../../exports))*" -type d -exec echo rm -r '{}' \;
    popd

    pushd preview/archive/summary
        XZ_OPT=-e9 find -maxdepth 1 -not -path '.' -type d -newer $(ls -At *.tar.xz | head -n 1) -exec echo XZ_OPT=-e9 tar -cvJf '{}.tar.xz' '{}' \;
        chown nginx:nginx *.tar.xz
        # remove previous years
        find -maxdepth 1 -not -path '.' -not -path "*$(date +%Y)-*" -type d -exec echo rm -r '{}' \+
        # remove all the but most recent 4 folders
        find -maxdepth 1 -not -path '.' -type d | sort -u | head -n -4 | xargs echo rm -r
    popd
#+end_src
*** elsewhere
#+begin_src bash
pushd /path/to/backup
rsync -z -v -r --links -e ssh cassava:/var/www/sparc sparc-$(date -I)
# export is big, so probably only keep two of these around, current and previous, copy current locally and then rsync into it again
rsync -z -v -r --links -e ssh cassava-sparc:/var/lib/sparc/.local/share/sparcur/export sparcur-export-$(date -I)
#+end_src

#+begin_src bash
pushd /path/to/backup
pushd sparc-*/sparc/archive/exports
find -maxdepth 1 -not -path '.' -type d -exec tar -cvJf '{}.tar.xz' '{}' \;
find -maxdepth 1 -not -path '.' -type d -exec rm -r '{}' \;
popd
pushd sparc-*/sparc/preview/archive/exports
find -maxdepth 1 -not -path '.' -type d -exec tar -cvJf '{}.tar.xz' '{}' \;
find -maxdepth 1 -not -path '.' -type d -exec rm -r '{}' \;
popd
#+end_src

** Other random commands
*** Duplicate top level and ./.operations/objects
:PROPERTIES:
:VISIBILITY: folded
:END:
# TODO upgrade this into backup and duplication
#+begin_src bash
function sparc-copy-pull () {
    : ${SPARC_PARENT:=${HOME}/files/blackfynn_local/}
    local TODAY=$(date +%Y%m%d)
    pushd ${SPARC_PARENT} &&
        mv SPARC\ Consortium "SPARC Consortium_${TODAY}" &&
        rsync -ptgo -A -X -d --no-recursive --exclude=* "SPARC Consortium_${TODAY}/"  SPARC\ Consortium &&
        mkdir SPARC\ Consortium/.operations &&
        mkdir SPARC\ Consortium/.operations/trash &&
        rsync -X -u -v -r "SPARC Consortium_${TODAY}/.operations/objects" SPARC\ Consortium/.operations/ &&
        pushd SPARC\ Consortium &&
        spc pull || echo "spc pull failed"
    popd
    popd
}
#+end_src
*** Simplified error report
:PROPERTIES:
:VISIBILITY: folded
:END:
#+CAPTION: simplified error report
#+begin_src bash
jq -r '[ .datasets[] |
         {id: .id,
          name: .meta.folder_name,
          se: [ .status.submission_errors[].message ] | unique,
          ce: [ .status.curation_errors[].message   ] | unique } ]' curation-export.json
#+end_src
*** File extensions
:PROPERTIES:
:VISIBILITY: folded
:END:
**** List all file extensions
Get a list of all file extensions.
#+begin_src bash
find -type l -o -type f | grep -o '\(\.[a-zA-Z0-9]\+\)\+$' | sort -u
#+end_src
**** Get ids with files matching a specific extension
Arbitrary information about a dataset with files matching a pattern.
The example here gives ids for all datasets that contain xml files.
Nesting =find -exec= does not work so the first pattern here uses shell
globing to get the datasets.
#+begin_src bash
function datasets-matching () {
    for d in */; do
        find "$d" \( -type l -o -type f \) -name "*.$1" \
        -exec getfattr -n user.bf.id --only-values "$d" \; -printf '\n' -quit ;
    done
}
#+end_src
**** Fetch files matching a specific pattern
Fetch files that have zero size (indication that fetch is broken).
#+begin_src bash
find -type f -name '*.xml' -empty -exec spc fetch {} \+
#+end_src
*** Sort of manifest generation
:PROPERTIES:
:VISIBILITY: folded
:END:
This is slow, but prototypes functionality useful for the curators.
#+begin_src bash
find -type d -not -name 'ephys' -name 'ses-*' -exec bash -c \
'pushd $1 1>/dev/null; pwd >> ~/manifest-stuff.txt; spc report size --tab-table ./* >> ~/manifest-stuff.txt; popd 1>/dev/null' _ {} \;
#+end_src
*** Path ids
This one is fairly slow, but is almost certainly i/o limited due to having to read the xattrs.
Maintaining the backup database of the mappings would make this much faster.
#+begin_src bash
# folders and files
find . -not -type l -not -path '*operations*' -exec getfattr -n user.bf.id --only-values {} \; -print
# broken symlink format, needs work, hard to parse
find . -type l -not -path '*operations*' -exec readlink -n {} \; -print
#+end_src
*** Path counts per dataset
#+begin_src bash
for d in */; do printf "$(find "${d}" -print | wc -l) "; printf "$(getfattr --only-values -n user.bf.id "${d}") ${d}\n" ; done | sort -n
#+end_src
*** Debug units serialization
Until we fix compound units parsing for the round trip we might
accidentally encounter and error along the lines of
=ValueError: Unit expression cannot have a scaling factor.=
#+begin_src bash
jq -C '.. | .units? // empty' /tmp/curation-export-*.json | sort -u
#+end_src
*** protocols cache
#+begin_src bash
pushd ~/.cache/idlib
mv protocol_json protocol_json-old
# run export
find protocol_json -size -2 -exec cat {} \+
# check to make sure that there weren't any manually provided caches
find protocol_json -size -2 -execdir cat ../protocol_json-old/{} \;
#+end_src
*** clean up org folders
:PROPERTIES:
:CREATED:  [2022-06-22 Wed 21:52]
:END:
THIS COMMAND IS DANGEROUS ONLY RUN IT IN =SPARC Consortium= folders that you want to nuke.
#+begin_src bash :eval never
find -maxdepth 1 -type d -not -name '.operations' -not -name '.' -exec rm -r {} \;
#+end_src
*** clean up broken symlinks in temp-upstream
:PROPERTIES:
:CREATED:  [2022-06-22 Wed 21:52]
:END:
Unfortunately keeping these around causes inode exhaustion issues.
Very slow, but only needs to be run once per system since the code has
been updated to do this during the transitive unsymlink.
#+begin_src python
from sparcur.paths import Path
here = Path.cwd()
here = Path('/var/lib/sparc/files/sparc-datasets-test')
bs = [
    rc
    for c in here.children
    for rd in (c / 'SPARC Consortium' / '.operations' / 'temp-upstream').rchildren_dirs
    for rc in rd.children
    if rc.is_broken_symlink()]
_ = [b.unlink() for b in bs]
#+end_src
*** clean up empty folders in temp-upstream
We already unlink the broken symlinks after completing the swap when
we pull a fresh copy, however when there are lots of changes the
folders themselves start to add up, so consider cleaning those out too
since mainly the use case is to keep a record of the metadata files
from a given date.

#+begin_src bash
pushd ~/files/sparc-datasets-test

# list empty directories in temp-upstream add -delete to remove them
find */SPARC/.operations/temp-upstream -type d -empty
#+end_src
*** clean up folders with old dataset names
This preserves the old =.operations= folders in =SPARC Consortium= for the time being.
#+begin_src bash
pushd ~/files/sparc-datasets-test

# shows how much space can be recovered by removing old dataset folders
find */SPARC*/ -mindepth 1 -maxdepth 1 -type d -not -name '.operations' -not -exec sh -c 'test "$(readlink "${1}"/../../dataset)" = "${1#*/}"' _ {} \; -exec du -hd0 --total {} \+ | sort -h

# sh -c 'echo "${1#*/}"' _ {} \;  # remove everything before the first slash

# actually delete, DO NOT PASTE THE OUTPUT TO RUN!!! there are spaces !!! remove the echo '#' bit
find */SPARC*/ -mindepth 1 -maxdepth 1 -type d -not -name '.operations' -not -exec sh -c 'test "$(readlink "${1}"/../../dataset)" = "${1#*/}"' _ {} \; -exec echo '#' rm -rf {} \;
#+end_src

*** dedupe =.operations/objects=
Check to see if objects already exist in =SPARC/.operations/objects=
and if not move them there from =SPARC Consortium/.operations/objects=
leaving duplicates to be removed.
#+begin_src bash
pushd ~/files/sparc-datasets-test

# list files that would be moved because they are not in SPARC/.objects
find */SPARC\ Consortium/.operations/objects -type f -not -exec bash -c 'target="${1/\ Consortium}"; test -f "${target}"' _ {} \; -print

# see the distribution of sizes for files that would be moved
find */SPARC\ Consortium/.operations/objects -type f -not -exec bash -c 'target="${1/\ Consortium}"; test -f "${target}"' _ {} \; -exec ls -alhS {} \+

# actually move the files, when running for real remove the echos AGAIN --- DO NOT PASTE
find */SPARC\ Consortium/.operations/objects -type f -exec bash -c 'target="${1/\ Consortium}"; test -f "${target}" || { echo mkdir -p "${target%/*}" && echo mv "${1}" "${target}"; }' _ {} \;

# internal consistency check (usually detects issues coming from upstream)
find */SPARC/.operations/objects -type f -exec spc meta --only-diff {} \+

# see if there are any cases where the files are not the same
find */SPARC\ Consortium/.operations/objects -type f -not -exec bash -c 'target="${1/\ Consortium}"; test -f "${target}" && { sha256sum "${1}" | sed "s/ Consortium//" | sha256sum --check --status; } ' _ {} \; -print

# delete files where the target exists (make sure all files are actually identical) change -print to -delete when ready to go for real
find */SPARC\ Consortium/.operations/objects -type f -exec bash -c 'target="${1/\ Consortium}"; test -f "${target}"' _ {} \; -print
#+end_src

*** clean up old =SPARC Consortium= folders
#+begin_src bash
pushd ~/files/sparc-datasets-test

# get modified dates for all consort variants
find -mindepth 3 -maxdepth 3 -path '*SPARC\ Consortium*' -not -name '.operations' -exec ls -alhtrd {} \+

# SPARC Consortium only cases
find -maxdepth 2 -type d -name 'SPARC*' | sort -u | grep Consort -B1 | grep -v -- '--' | sort | cut -d\/ -f2 | uniq -u | xargs -I[] find [] -mindepth 1 -maxdepth 1 -type d | grep Consort | cut -d\/ -f1

# no consort cases
find -maxdepth 2 -type d -name 'SPARC*' | sort -u | grep Consort -B1 | grep -v -- '--' | sort | cut -d\/ -f2 | uniq -u | xargs -I[] find [] -mindepth 1 -maxdepth 1 -type d | grep -v Consort | cut -d\/ -f1

# only both SPARC and SPARC Consortium cases
find -mindepth 2 -maxdepth 2 -type d -name 'SPARC' -exec test -d {}/../SPARC\ Consortium \; -exec ls {}/.. \;

# get sizes of the consort folders
find -mindepth 2 -maxdepth 2 -type d -name 'SPARC' -exec test -d {}/../SPARC\ Consortium \; -exec du -hd0 {}\ Consortium \; | sort -h
#+end_src

*** REVA merged ttl
#+name: reva-merged-datasets
| dataset-id                                     | short-id         |
|------------------------------------------------+------------------|
| N:dataset:e225ea82-54b5-457f-ad3d-faa640eb13be | f013             |
| N:dataset:7a542123-ce8d-4f53-9b74-d259958db1ea | f012             |
| N:dataset:1c929cf2-213a-45ed-a867-47a7864c83eb | f011             |
| N:dataset:09a09632-a425-4802-b5f2-464c9177ef41 | f010             |
| N:dataset:614ca71d-863f-4bb3-959e-e74e814d8e1a | f018             |
| N:dataset:bd90e81f-fb33-40ce-93e1-44875efde91b | f014             |
| N:dataset:bd90e81f-fb33-40ce-93e1-44875efde91b | f015             |
| N:dataset:c5edefba-732d-4c5e-a66c-9081d6885b9e | f017             |
| N:dataset:c5edefba-732d-4c5e-a66c-9081d6885b9e | f016             |
| N:dataset:2a3d01c0-39d3-464a-8746-54c9d67ebe0f | f006             |
| N:dataset:5c7b9f9d-eeda-4370-a5b8-892020f863c2 | f007             |
| N:dataset:33a9f81e-1deb-4c2c-922a-f9eac47ed3e5 | f008             |
| N:dataset:ec6ad74e-7b59-409b-8fc7-a304319b6faf | f003             |
| N:dataset:aa43eda8-b29a-4c25-9840-ecbd57598afc | f001             |
| N:dataset:bc4cc558-727c-4691-ae6d-498b57a10085 | f002             |
| N:dataset:04a5fed9-7ba6-4292-b1a6-9cab5c38895f | f004             |
| N:dataset:a8b2bdc7-54df-46a3-810e-83cdf33cfc3a | f005             |
| N:dataset:9d2b7a97-1923-4880-a3cc-3a85c8720839 | f009             |
| N:dataset:fb1cbd05-4320-4d8b-ac3a-44f1fe810718 | MicroCT          |
| N:dataset:f8c2985b-1fd3-4db8-90f4-3ce72357c949 | Excised-Nerve-CT |
| N:dataset:47723323-5c44-4656-8c01-da5bd39ff053 | Anatomy          |
| N:dataset:17c63dac-4f07-436e-98b0-45457d0571a1 | Histology        |
| N:dataset:3da605ff-1d3b-47ea-82c3-b99395ceedde | 3D-Nerve-Tracing |

not ready/missing
| N:dataset:a80b6ba1-f7e4-42dc-8c78-b7573c4c798a | MRI              |
| N:dataset:0affd8ff-6670-4ca5-b125-70f1eeb032f8 | MUSE             |

#+begin_src elisp :var data-in=reva-merged-datasets() :results none :lexical yes
(let ((local t)
      (user "")
      (n 0))
 (with-temp-buffer
   (insert "@prefix ilxtr: <http://uri.interlex.org/tgbugs/uris/readable/> .\n")
   (mapcar
    (lambda (row)
      (let* ((dataset-id (car row))
             (uuid (car (reverse (string-split dataset-id ":"))))
             (url (format "https://cassava.ucsd.edu/sparc/datasets/%s/LATEST/curation-export.ttl" uuid))
             (file (format "~%s/.local/share/sparcur/export/datasets/%s/LATEST/curation-export.ttl" user uuid))
             (url-or-file (if local file url))
             (new-min (point-max)))
        (goto-char (point-max)) ; withoutthis insert-file-contents will insert at 1
        (with-url-handler-mode
          (insert-file-contents url-or-file)
          (let ((new-max (point-max)))
            (setq n (1+ n))
            (message ":n %s :new-min %s :new-max %s" n new-min new-max )
            (evil-ex-substitute new-min new-max '("local:") (format "local%s:" n) '("g"))
            (evil-ex-substitute new-min new-max '("a owl:Ontology") "a ilxtr:EmbeddedOntology" '("g"))
            ;;(evil-ex-substitute (point-min) (point-max) '("local:") (format "local%s:" n) '("g"))
            ))))
    data-in)
   (goto-char (point-max))
   ;; FIXME somehow rdflib ttl parser complaining about bare / in curies which is allowed ffs
   (insert (format "\nilxtr:ontologies\\/reva-merge-test a owl:Ontology ; rdfs:label \"REVA dataset metadata merge %s\" .\n" (format-time-string "%Y-%m-%d" (current-time)))) ; TODO better metadata obvs e.g. via (defun (make-ontology-metadata ...) ..)
   ;; don't bother with this for now because we will have to run it later anyway
   ;;(shell-command-on-region (point-min) (point-max) "ttlfmt -f ttl" (current-buffer) 'replace) ; slooow
   (write-file "/tmp/reva-merged.ttl")))
#+end_src

** datasets causing issues with fetching files
:PROPERTIES:
:CREATED:  [2022-02-08 Tue 13:58]
:END:
#+name: datasets-with-fetch-errors
#+begin_src bash :dir ~/files/sparc-datasets-test
find */SPARC\ Consortium/.operations/temp-upstream/ -type d -name '*-ERROR' | cut -d'/' -f 1 | sort -u
#+end_src

#+call: datasets-with-fetch-errors() :dir ~/files/sparc-datasets

#+begin_src bash
python -m sparcur.simple.retrieve --jobs 1 --sparse-limit -1 --parent-parent-path . --dataset-id $1
pushd $1
spc export 
#+end_src
** viewing single dataset logs
#+begin_src bash
pushd ~/.cache/log/sparcur/datasets
find -name stdout.log -printf "%T@ %Tc %p\n" | sort -n
less -R $_some_path
#+end_src
** fixing feff issues
#+begin_src python
from sparcur.datasets import Tabular
from sparcur.paths import Path
p = Path('dataset_description.xlsx')
t = Tabular(p)
hrm1 = list(t.xlsx1())
hrm2 = list(t.xlsx2())
#+end_src
look for =\ufeff= at the start of strings and then use e.g. vim to
open and edit the file removing it from the offending strings
** View logs for failed single dataset exports
Run the function, paste in the ids under failed and hit enter.
#+begin_src bash
function review-failed () {
    local paths _id
    paths=()
    while read _id; do
        paths+=(~/.cache/log/sparcur/datasets/${_id}/LATEST/stdout.log)
        if [ -z $_id ]; then break; fi
    done
    less -R ${paths[@]}
}
#+end_src

From curl instead of paste.
#+begin_src bash
function review-failed-curl () {
    local paths
    paths=()
    for _id in ${@} ; do
        paths+=(~/.cache/log/sparcur/datasets/${_id}/LATEST/stdout.log)
        if [ -z $_id ]; then break; fi
    done
    less -R ${paths[@]}
}
#+end_src

Usage.
#+begin_src bash
review-failed-curl $(curl https://cassava.ucsd.edu/sparc/pipelines/failed | jq -r '.failed[]' | sed 's/N:dataset://')
#+end_src
** fixing missing file metadata
#+begin_src bash
find -type f -exec sh -c '[[ "$(getfattr -d $1)" = "" ]] || exit 1' _ {} \; -exec python -m sparcur.cli meta --fake --meta-from-local {} \+
#+end_src

** COMMENT Get data
:PROPERTIES:
:CUSTOM_ID: get-data
:VISIBILITY: folded
:END:
If you have never retrieved the data before run.
#+CAPTION: first time per local network
#+BEGIN_SRC bash :results none
pushd ~/files/blackfynn_local/
spc clone ${SPARC_ORG_ID} # initialize a new repo and pull existing structure
scp refresh -f
spc fetch  # actually download files
spc find -n '*.xlsx' -n '*.csv' -n '*.tsv' -n '*.msexcel'  # see what to fetch
spc find -n '*.xlsx' -n '*.csv' -n '*.tsv' -n '*.msexcel'-f  # fetch
spc find -n '*.xlsx' -n '*.csv' -n '*.tsv' -n '*.msexcel'-f -r 10  # slow down you are seeing errors!
#+END_SRC

#+CAPTION: unfriendly refersh
#+BEGIN_SRC bash :results none
ls -Q | xargs -P10 -r -n 1 sh -c 'spc refresh -r 4 "${1}"'
#+END_SRC

#+CAPTION: friendly refersh
#+BEGIN_SRC bash :results none
find -maxdepth 1 -type d -name '[C-Z]*' -exec spc refresh -r 8 {} \;
#+END_SRC

#+CAPTION: find any stragglers
#+BEGIN_SRC bash :results none
find \( -name '*.xlsx' -o -name '*.csv' -o -name '*.tsv' \) -exec ls -hlS {} \+
#+END_SRC

Open the dataset page for all empty directories in the browser.
#+begin_src bash
find -maxdepth 1 -type d -empty -exec spc pull {} \+
find -maxdepth 1 -type d -empty -exec spc meta -u --browser {} \+
#+end_src

# temp fix for summary making folders when it should skip
#+CAPTION: clean up empty directories
#+BEGIN_SRC bash :results none
find -maxdepth 1 -type d -empty -exec rmdir {} \;
#+END_SRC

#+caption: copy sparse for full clone
#+begin_src bash
find -maxdepth 1 -type d -exec getfattr -n user.bf.id \;
#+end_src

Pull local copy of data to a new computer. Note the double escape needed for the space.
#+BEGIN_SRC bash :results none :eval never
rsync -X -u -v -r -e ssh ${REMOTE_HOST}:/home/${DATA_USER}/files/blackfynn_local/SPARC\\\ Consortium ~/files/blackfynn_local/
#+END_SRC
=-X= copy extended attributes
=-u= update files
=-v= verbose
=-r= recursive
=-e= remote shell to use
** COMMENT Fetch missing files
:PROPERTIES:
:VISIBILITY: folded
:END:
fetching a whole dataset or a subset of a dataset
=spc ** -f=
** COMMENT Export
:PROPERTIES:
:VISIBILITY: folded
:END:
#+CAPTION: export everything
#+BEGIN_SRC bash
pushd ${SPARCDATA}
spc export
popd
#+END_SRC

Setup as root
#+begin_src bash :eval never
mkdir -p /var/www/sparc/sparc/archive/exports/
chown -R nginx:nginx /var/www/sparc
#+end_src

#+name: &sparc-export-to-server-function
#+CAPTION: copy export to server location, run as root
#+BEGIN_SRC bash :eval never
# export vs exports, no wonder this is so confusing >_<
function sparc-export-to-server () {
    : ${SPARCUR_EXPORTS:=/var/lib/sparc/.local/share/sparcur/export}
    EXPORT_BASE=${SPARCUR_EXPORTS}/N:organization:618e8dd9-f8d2-4dc4-9abb-c6aaab2e78a0/integrated/
    FOLDERNAME=$(readlink ${EXPORT_BASE}/LATEST)
    FULLPATH=${EXPORT_BASE}/${FOLDERNAME}
    pushd /var/www/sparc/sparc
    cp -a "${FULLPATH}" archive/exports/ && chown -R nginx:nginx archive && unlink exports ; ln -sT "archive/exports/${FOLDERNAME}" exports
    popd
    echo Export complete. Check results at:
    echo fill-in-the-url-here
}
#+END_SRC
** COMMENT Export v3
#+begin_src bash
function preview-sparc-export-to-server () {
    : ${SPARCUR_EXPORTS:=/var/lib/sparc/.local/share/sparcur/export}
    EXPORT_BASE=${SPARCUR_EXPORTS}/618e8dd9-f8d2-4dc4-9abb-c6aaab2e78a0/integrated/
    FOLDERNAME=$(readlink ${EXPORT_BASE}/LATEST)
    FULLPATH=${EXPORT_BASE}/${FOLDERNAME}
    pushd /var/www/sparc/sparc/preview
    cp -a "${FULLPATH}" archive/exports/ && chown -R nginx:nginx archive && unlink exports ; ln -sT "archive/exports/${FOLDERNAME}" exports
    popd
    echo Export complete. Check results at:
    echo https://cassava.ucsd.edu/sparc/preview/archive/exports/${FOLDERNAME}
}
#+end_src

The shared information on the file system is evil because there may be multiple processes.
The way to mitigate the issue is to run everything locally with a read only local cache for certain files.
# FIXME this can fail if files aren't quite where we expect them
#+begin_src bash :eval never
function preview-export-rest () {
    local DATE1=${1} # 2021-03-09T17\:26\:54\,980772-08\:00  # from spc export
    local DATE2=${2} # 2021-03-09T164046,487692-0800  # from the path created by sparc-get-all-remote-data
    cp -a /var/lib/sparc/.local/share/sparcur/export/protcur/LATEST/protcur.ttl /var/www/sparc/sparc/preview/archive/exports/${DATE1}/  # this may not update and should be versioned independently
    cp -a /var/lib/sparc/files/${DATE2}/exports/datasets /var/www/sparc/sparc/preview/archive/exports/${DATE1}/path-metadata  # NOTE these will not change unless the files or the code/format change
    chown -R nginx:nginx /var/www/sparc/sparc/preview/archive/exports/${DATE1}/
}
#+end_src

#+begin_src bash :dir "/ssh:cassava-sparc:" :eval never
# git repos are in ~/files/venvs/sparcur-dev/git
# use the development pull code
source ~/files/venvs/sparcur-dev/bin/activate
source ~/files/venvs/sparcur-dev/git/sparc-curation/bin/pipeline-functions.sh
export PYTHONBREAKPOINT=0  # ensure that breakpoints do not hang export
pushd ~/files/
PARENT_PATH=$(sparc-time-friendly)
sparc-get-all-remote-data \
    --symlink-objects-to ~/files/blackfynn_local/SPARC\ Consortium_20200601/.operations/objects/ \
    --parent-path "${PARENT_PATH}"
pushd "${PARENT_PATH}/SPARC Consortium"
spc export
find -maxdepth 1 -type d -not -path '.operations*' -not -path '.' -print0 | \
     xargs -0 -I{} -P8 -r -n 1 python -m sparcur.simple.path_metadata_validate --export-path ../exports/ {}
pushd ~/.local/share/sparcur/export/618*/integrated/LATEST/; python -m sparcur.export.published; popd
echo "${PARENT_PATH}"
unset PARENT_PATH
#+end_src

An example. Get =DATE1= from =spc export= or from the output of
=preview-sparc-export-to-server=. Get =DATE2= from the file system
path created by the initial call to =sparc-get-all-remote-data=.
Export time is usually later than parent time.
#+begin_src bash :dir /ssh:cassava|sudo:cassava :eval never
preview-sparc-export-to-server
preview-export-rest ${EXPORT_PATH_TIME} ${PARENT_PATH_TIME}
#+end_src
** COMMENT Export v4
:PROPERTIES:
:CUSTOM_ID: export-v4
:END:

#+begin_src bash
source ~/files/venvs/sparcur-dev/bin/activate
python -m sparcur.simple.combine &&
python -m sparcur.simple.disco ~/.local/share/sparcur/export/summary/618*/LATEST/curation-export.json &&
echo Export complete. Check results at: ;
echo https://cassava.ucsd.edu/sparc/preview/archive/summary/$(readlink ~/.local/share/sparcur/export/summary/618*/LATEST)
#+end_src

*** COMMENT deprecated
Then as root run =combine-sparc-export-to-server=, which is defined as
follows and should be in =~/.bashrc=.
#+begin_src bash
# THIS IS NO LONGER NEEDED DO NOT USE IT
function combine-sparc-export-to-server () {
    : ${SPARCUR_EXPORTS:=/var/lib/sparc/.local/share/sparcur/export}
    FULLPATH=$(readlink -f ${SPARCUR_EXPORTS}/summary/618*/LATEST)
    FOLDERNAME=$(basename "${FULLPATH}")
    pushd /var/www/sparc/sparc/preview
    ln -s "${FULLPATH}" "archive/exports/${FOLDERNAME}" \
    && unlink exports \
    ; ln -s "archive/exports/${FOLDERNAME}" exports
    popd
    echo Export complete. Check results at:
    echo https://cassava.ucsd.edu/sparc/preview/archive/exports/${FOLDERNAME}
}
#+end_src
** COMMENT Export published
Generate =curation-export-published.ttl= for existing exports.

#+begin_src bash
pushd /var/www/sparc/sparc/preview/archive/exports
find -maxdepth 1 -type d -exec sudo chown $UID:$UID {} \;
find -name curation-export.ttl -execdir python -m sparcur.export.published \;
find -maxdepth 1 -type d -exec sudo chown -R nginx:nginx {} \;
popd
#+end_src

* SCKAN
See the developer guide section on [[file:./developer-guide.org::#sckan][SCKAN]].
* SODA
Have to clone [[https://github.com/bvhpatel/SODA][SODA]] and fetch the files for testing.
#+header: :var parent_folder="~/files/blackfynn_local/"
#+header: :var path="./SPARC Consortium/The effect of gastric stimulation location on circulating blood hormone levels in fasted anesthetized rats/source/pool-r1009"
#+begin_src python :dir ~/git/SODA/src/pysoda :results drawer output
from pprint import pprint
import pysoda
from sparcur.paths import Path
p = Path(parent_folder, path).expanduser().resolve()
children = list(p.iterdir())
blob = pysoda.create_folder_level_manifest(
    {p.resolve().name: children},
    {k.name + '_description': ['some description'] * len(children)
     for k in [p] + list(p.iterdir())})
manifest_path = Path(blob[p.name][-1])
manifest_path.xopen()
pprint(manifest_path)
#+end_src
* Developer
See also the [[file:./developer-guide.org][sparcur developer guild]]
** Releases
:PROPERTIES:
:VISIBILITY: folded
:END:
*** DatasetTemplate
Clean up existing files.

#+begin_src bash
pushd ~/git/sparc-curation/resources
pypy3 -m sparcur.simple.clean_metadata_files --for-template clean --cleaned-output-path dt_clean DatasetTemplate
cp dt_cleaned/*.xlsx DatasetTemplate/
#+end_src

Commit any changes and push to master.

Generate diffs against the previous tag and then view with =less=.
#+begin_src bash :results none
pushd ~/git/CLEANROOM/sparc-curation/resources/DatasetTemplate
[ -d ../csvs ]  || mkdir ../csvs
[ -d ../diffs ] || mkdir ../diffs
lasttag=$(git tag --sort=taggerdate --list dataset-template* | tail -n1)
dtver="${lasttag##*-}"
for f in $(ls *.xlsx); do
git show dataset-template-3.0.0:resources/DatasetTemplate/"${f}" | xlsx2csv - ../csvs/"${f%%.*}-${dtver}.csv"
xlsx2csv "${f}" ../csvs/"${f%%.*}.csv"
git diff --word-diff --word-diff-regex=. --no-index --color=always -- ../csvs/"${f%%.*}-${dtver}.csv" ../csvs/"${f%%.*}.csv" > ../diffs/"${f%%.*}.patch"
done
# less ../diffs/*
popd
#+end_src

#+begin_src bash
make-template-zip () {
    template_type="${1}"
    local CLEANROOM=/tmp/cleanroom/
    mkdir ${CLEANROOM} || return 1
    pushd ${CLEANROOM}
    git clone https://github.com/SciCrunch/sparc-curation.git &&
    pushd ${CLEANROOM}/sparc-curation/resources
    # TODO path to spec file for various templates (see sparcur.simple.clean_metadata_files and datasets.Tabular._openpyxl_fixes)
    python -m sparcur.simple.clean_metadata_files --for-template ${template_type} --cleaned-output-path "dt_${template_type}" DatasetTemplate
    cp "dt_${template_type}"/*.xlsx DatasetTemplate/
    zip -x '*.gitkeep' -x '*/curation.xlsx' -x '*/aux*' -r DatasetTemplate.zip DatasetTemplate
    mv DatasetTemplate.zip ${CLEANROOM}
    popd
    rm -rf ${CLEANROOM}/sparc-curation
    popd
}
make-template-zip default
#+end_src

Once that is done open /tmp/cleanroom/DatasetTemplate.zip in =file-roller= or similar
and make sure everything is as expected.

Create the GitHub release. The tag name should have the format =dataset-template-1.1= where
the version number should match the metadata version embedded in
[[file:../resources/DatasetTemplate/dataset_description.xlsx][dataset_description.xlsx]].
Minor versions such as =dataset-template-1.2.1= are allowed.

Attach =${CLEANROOM}/DatasetTemplate.zip= as a release asset.

Inform curation so they can notify the community.
** Getting to know the codebase
:PROPERTIES:
:VISIBILITY: folded
:END:
Use =inspect.getclasstree= along with =pyontutils.utils.subclasses=
to display hierarchies of classes.
#+begin_src python :results output code :wrap "example python"
from inspect import getclasstree
from pyontutils.utils import subclasses
from IPython.lib.pretty import pprint

# classes to inspect
import pathlib
from sparcur import paths

def class_tree(root):
    return getclasstree(list(subclasses(root)))

pprint(class_tree(pathlib.PurePosixPath))
#+end_src

** Viewing logs
:PROPERTIES:
:VISIBILITY: folded
:END:
View the latest log file with colors using =less=.
#+begin_src bash
less -R $(ls -d ~sparc/files/blackfynn_local/export/log/* | tail -n 1)
#+end_src
For a permanent fix for =less= add
#+begin_src bash
alias less='less -R'
#+end_src

** Debugging fatal pipeline errors
:PROPERTIES:
:VISIBILITY: folded
:END:
You have an error!
#+begin_src python
maybe_size = c.cache.meta.size  # << AttributeError here
#+end_src

Modify to wrap code
#+begin_src python
try:
    maybe_size = c.cache.meta.size
except AttributeError as e:
    breakpoint()  # << investigate error
#+end_src

Temporary squash by logging as an exception with optional explanation
#+begin_src python
try:
    maybe_size = c.cache.meta.size
except AttributeError as e:
    log.exception(e)
    log.error(f'explanation for error and local variables {c}')
#+end_src
